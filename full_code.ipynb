{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9353702,"sourceType":"datasetVersion","datasetId":5670231},{"sourceId":9414413,"sourceType":"datasetVersion","datasetId":5714747},{"sourceId":196897911,"sourceType":"kernelVersion"},{"sourceId":114649,"sourceType":"modelInstanceVersion","modelInstanceId":96269,"modelId":120451}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Definicja etykiet emocji\nemotion_labels = ['Neutral', 'Happiness', 'Sadness', 'Surprise', 'Fear', 'Disgust', 'Anger']\n\n# Sprawdzenie dostępności GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Ustawienie ścieżek\ndata_path = \"/kaggle/input/raf-db-zmienione-labele\"\ntrain_dir = os.path.join(data_path, \"train\")\ntest_dir = os.path.join(data_path, \"test\")\n\n# Konfiguracja parametrów\nnum_classes = len(emotion_labels)\nbatch_size = 64\nnum_epochs = 20\nlearning_rate = 0.01\nmomentum = 0.9\nweight_decay = 1e-4\n\n# Normalizacja obrazów\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Przygotowanie transformacji dla danych\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.ToTensor(),\n    normalize,\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize,\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T13:32:17.487342Z","iopub.execute_input":"2024-09-16T13:32:17.487704Z","iopub.status.idle":"2024-09-16T13:32:17.499820Z","shell.execute_reply.started":"2024-09-16T13:32:17.487671Z","shell.execute_reply":"2024-09-16T13:32:17.498941Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\n\nclass LocalFeatureExtractor(nn.Module):\n    def __init__(self, inplanes, planes):\n        super(LocalFeatureExtractor, self).__init__()\n        self.conv1_1 = nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=2, padding=1, groups=inplanes, bias=False)\n        self.bn1_1 = nn.BatchNorm2d(inplanes)\n        self.conv1_2 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.bn1_2 = nn.BatchNorm2d(planes)\n\n        self.conv2_1 = nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=2, padding=1, groups=inplanes, bias=False)\n        self.bn2_1 = nn.BatchNorm2d(inplanes)\n        self.conv2_2 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.bn2_2 = nn.BatchNorm2d(planes)\n\n        self.conv3_1 = nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=2, padding=1, groups=inplanes, bias=False)\n        self.bn3_1 = nn.BatchNorm2d(inplanes)\n        self.conv3_2 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.bn3_2 = nn.BatchNorm2d(planes)\n\n        self.conv4_1 = nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=2, padding=1, groups=inplanes, bias=False)\n        self.bn4_1 = nn.BatchNorm2d(inplanes)\n        self.conv4_2 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, bias=False)\n        self.bn4_2 = nn.BatchNorm2d(planes)\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        patch_11 = x[:, :, 0:28, 0:28]\n        patch_21 = x[:, :, 28:56, 0:28]\n        patch_12 = x[:, :, 0:28, 28:56]\n        patch_22 = x[:, :, 28:56, 28:56]\n\n        out_1 = self.relu(self.bn1_2(self.conv1_2(self.relu(self.bn1_1(self.conv1_1(patch_11))))))\n        out_2 = self.relu(self.bn2_2(self.conv2_2(self.relu(self.bn2_1(self.conv2_1(patch_21))))))\n        out_3 = self.relu(self.bn3_2(self.conv3_2(self.relu(self.bn3_1(self.conv3_1(patch_12))))))\n        out_4 = self.relu(self.bn4_2(self.conv4_2(self.relu(self.bn4_1(self.conv4_1(patch_22))))))\n\n        out1 = torch.cat([out_1, out_2], dim=2)\n        out2 = torch.cat([out_3, out_4], dim=2)\n        out = torch.cat([out1, out2], dim=3)\n\n        return out\n\n# Implementacja mechanizmu uwagi\nclass AttentionModule(nn.Module):\n    def __init__(self, in_channels):\n        super(AttentionModule, self).__init__()\n        self.attention = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // 16, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(in_channels // 16, in_channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        attention_weights = self.attention(x)\n        return x * attention_weights\n\n# Hybrydowy model EfficientFace-ResNet\nclass EfficientFaceResNet(nn.Module):\n    def __init__(self, num_classes):\n        super(EfficientFaceResNet, self).__init__()\n        self.resnet = models.resnet50(pretrained=True)\n        self.local_feature_extractor = LocalFeatureExtractor(3, 116)\n        self.attention1 = AttentionModule(256)\n        self.attention2 = AttentionModule(512)\n        self.attention3 = AttentionModule(1024)\n        self.attention4 = AttentionModule(2048)\n        \n        # Dodajemy warstwę konwolucyjną do dostosowania wymiarów cech lokalnych\n        self.local_feature_adapter = nn.Conv2d(116, 2048, kernel_size=1)\n        \n        self.fc = nn.Linear(2048, num_classes)\n\n    def forward(self, x):\n        # Local Feature Extractor\n        local_features = self.local_feature_extractor(x)\n\n        # ResNet layers with attention\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)\n\n        x = self.resnet.layer1(x)\n        x = self.attention1(x)\n\n        x = self.resnet.layer2(x)\n        x = self.attention2(x)\n\n        x = self.resnet.layer3(x)\n        x = self.attention3(x)\n\n        x = self.resnet.layer4(x)\n        x = self.attention4(x)\n\n        # Dostosuj wymiary cech lokalnych i połącz z cechami globalnymi\n        adapted_local_features = self.local_feature_adapter(local_features)\n        adapted_local_features = F.interpolate(adapted_local_features, size=x.size()[2:], mode='bilinear', align_corners=False)\n        x = x + adapted_local_features\n\n        x = self.resnet.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Inicjalizacja modelu\nmodel = EfficientFaceResNet(num_classes=7).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T13:32:17.501984Z","iopub.execute_input":"2024-09-16T13:32:17.502316Z","iopub.status.idle":"2024-09-16T13:32:18.101859Z","shell.execute_reply.started":"2024-09-16T13:32:17.502282Z","shell.execute_reply":"2024-09-16T13:32:18.100766Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Wczytanie danych treningowych i testowych\ntrain_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n# Wyświetlenie przykładowych obrazów\ndef show_samples(dataset, num_samples=7):\n    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n    for i in range(num_samples):\n        idx = np.random.randint(len(dataset))\n        img, label = dataset[idx]\n        img = img.permute(1, 2, 0).numpy()\n        img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n        axes[i].imshow(img)\n        axes[i].set_title(emotion_labels[label])\n        axes[i].axis('off')\n    plt.show()\n\nprint(\"Przykładowe obrazy treningowe:\")\nshow_samples(train_dataset)\nprint(\"Przykładowe obrazy testowe:\")\nshow_samples(test_dataset)\n\n# Inicjalizacja modelu\nmodel = model.to(device)\n\n# Inicjalizacja kryterium straty i optymalizatora\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=momentum, weight_decay=weight_decay)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-09-16T13:32:18.103622Z","iopub.execute_input":"2024-09-16T13:32:18.103986Z","iopub.status.idle":"2024-09-16T13:32:25.483289Z","shell.execute_reply.started":"2024-09-16T13:32:18.103951Z","shell.execute_reply":"2024-09-16T13:32:25.482530Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=False, delta=0):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss\n\nclass LRScheduler:\n    def __init__(self, optimizer, patience=5, min_lr=1e-6, factor=0.5):\n        self.optimizer = optimizer\n        self.patience = patience\n        self.min_lr = min_lr\n        self.factor = factor\n        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n                self.optimizer,\n                mode='min',\n                patience=self.patience,\n                factor=self.factor,\n                min_lr=self.min_lr,\n                verbose=True\n            )\n\n    def __call__(self, val_loss):\n        self.lr_scheduler.step(val_loss)\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res","metadata":{"execution":{"iopub.status.busy":"2024-09-16T13:32:25.485529Z","iopub.execute_input":"2024-09-16T13:32:25.485840Z","iopub.status.idle":"2024-09-16T13:32:25.501975Z","shell.execute_reply.started":"2024-09-16T13:32:25.485808Z","shell.execute_reply":"2024-09-16T13:32:25.501262Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    for images, targets in train_loader:\n        images, targets = images.to(device), targets.to(device)\n\n        outputs = model(images)\n        loss = criterion(outputs, targets)\n\n        acc1, = accuracy(outputs, targets, topk=(1,))\n        losses.update(loss.item(), images.size(0))\n        top1.update(acc1[0], images.size(0))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return losses.avg, top1.avg\n\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    with torch.no_grad():\n        for images, targets in val_loader:\n            images, targets = images.to(device), targets.to(device)\n\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n\n            acc1, = accuracy(outputs, targets, topk=(1,))\n            losses.update(loss.item(), images.size(0))\n            top1.update(acc1[0], images.size(0))\n\n    return losses.avg, top1.avg","metadata":{"execution":{"iopub.status.busy":"2024-09-16T13:32:25.503079Z","iopub.execute_input":"2024-09-16T13:32:25.503783Z","iopub.status.idle":"2024-09-16T13:32:25.521051Z","shell.execute_reply.started":"2024-09-16T13:32:25.503737Z","shell.execute_reply":"2024-09-16T13:32:25.520271Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, verbose=True)\nlr_scheduler = LRScheduler(optimizer)\n\n# Listy do przechowywania wyników\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\n\nbest_val_acc = 0.0\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    \n    # Trening\n    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n    train_losses.append(float(train_loss))  # Konwersja na float, jeśli to tensor\n    train_accuracies.append(float(train_acc))\n    \n    val_loss, val_acc = validate(model, test_loader, criterion, device)\n    val_losses.append(float(val_loss))  # Konwersja na float, jeśli to tensor\n    val_accuracies.append(float(val_acc))\n    \n    # Wydruk wyników\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n    \n    # Aktualizacja learning rate\n    lr_scheduler(val_loss)\n    \n    # Zapisywanie najlepszego modelu\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f'New best model saved with validation accuracy: {best_val_acc:.2f}%')\n    \n    # Early stopping\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n\n# Wczytanie najlepszego modelu\nmodel.load_state_dict(torch.load('best_model.pth'))\n\n# Ewaluacja na zbiorze testowym\nmodel.eval()\ntest_loss, test_acc = validate(model, test_loader, criterion, device)\nprint(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n\n# Macierz pomyłek\ny_true = []\ny_pred = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10,8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n# Zapisywanie modelu\ntorch.save(model.state_dict(), 'efficient_face_raf_db.pth')\nprint(\"Model został zapisany.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T13:32:25.522048Z","iopub.execute_input":"2024-09-16T13:32:25.522597Z","iopub.status.idle":"2024-09-16T14:31:34.268324Z","shell.execute_reply.started":"2024-09-16T13:32:25.522565Z","shell.execute_reply":"2024-09-16T14:31:34.267101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:31:34.270032Z","iopub.execute_input":"2024-09-16T14:31:34.270405Z","iopub.status.idle":"2024-09-16T14:31:34.963245Z","shell.execute_reply.started":"2024-09-16T14:31:34.270364Z","shell.execute_reply":"2024-09-16T14:31:34.962396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\ntest_loss, test_acc = validate(model, test_loader, criterion, device)\nprint(f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n\n# Macierz pomyłek\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ny_true = []\ny_pred = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(12,10))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=emotion_labels, yticklabels=emotion_labels)\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:31:34.964662Z","iopub.execute_input":"2024-09-16T14:31:34.965373Z","iopub.status.idle":"2024-09-16T14:32:03.741110Z","shell.execute_reply.started":"2024-09-16T14:31:34.965318Z","shell.execute_reply":"2024-09-16T14:32:03.740126Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zapisywanie całego modelu\ntorch.save(model, 'efficient_face_full_model.pth')\n\n# Zapisywanie tylko stanu modelu (zalecane)\ntorch.save(model.state_dict(), 'efficient_face_state_dict.pth')\n\nprint(\"Model został zapisany.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T14:32:03.744974Z","iopub.execute_input":"2024-09-16T14:32:03.745419Z","iopub.status.idle":"2024-09-16T14:32:04.279738Z","shell.execute_reply.started":"2024-09-16T14:32:03.745379Z","shell.execute_reply":"2024-09-16T14:32:04.278693Z"},"trusted":true},"outputs":[],"execution_count":null}]}